{
  "updated": "2026-02-14T01:35:00Z",
  "items": [
    {
      "id": "camera-extrinsic-calibration",
      "name": "Camera Extrinsic Calibration",
      "description": "Calibrate and provide the extrinsic transform (4x4 homogeneous matrix) between each camera frame and the robot base frame. This should be accessible via the SDK so frontend agents can convert 3D detections from camera coordinates to robot base coordinates for accurate manipulation.",
      "category": "sdk",
      "requested_by": "frank",
      "reason": "YOLO 3D segmentation returns object positions in camera frame, but to move the arm to pick up objects we need positions in robot base frame. Without this transform, we cannot reliably reach detected objects.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "grounded-sam2",
      "name": "Grounded SAM 2",
      "description": "Host Grounded SAM 2 as a backend service for open-vocabulary object detection and segmentation. Accepts an image and text prompts (e.g., 'red cup', 'screwdriver') and returns bounding boxes, segmentation masks, and confidence scores for the described objects. Should support both bounding-box detection (via Grounding DINO) and fine-grained mask segmentation (via SAM 2).",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "YOLO detection is limited to its fixed class vocabulary. For real-world manipulation tasks we need open-vocabulary perception â€” the ability to detect and segment arbitrary objects described in natural language. Grounded SAM 2 enables this by combining Grounding DINO's text-conditioned detection with SAM 2's high-quality segmentation.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    }
  ]
}
